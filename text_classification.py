# -*- coding: utf-8 -*-
"""Text Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J1_m4EHrvgU7Q4lT-E3SP3d1WNRbEmcL
"""

# Import the libraries
import csv
import pandas as pd
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
import matplotlib.pyplot as plt

# Initializing the parameters
vocab_size = 1000 # Max. no. of words to keep in the vocabulary
embedding_dim = 16 # Dimensionality of the word embeddings
max_length = 120 # Max. no. of tokens in a sentence
trunc_type='post' # Truncate extra words from the end of the sentence
padding_type='post' # Pad sentences with 0s at the end if shorter than max_length
oov_tok = "<OOV>" # Out-Of-Vocabulary token for unknown words
training_portion = .8 # Use 80% for training

# Initializing empty lists
sentences = []
labels = []

# Load stopwords
stop_words = set(stopwords.words('english'))

# Read CSV file
with open("/content/bbc-text.csv", 'r') as csvfile:
  reader = csv.reader(csvfile, delimiter=',')
  next(reader) # Skip header
  for row in reader:
    labels.append(row[0]) # Save first column in the labels list
    sentence = row[1] # Save second column in the sentences list

    # Remove stopwords from sentences
    for word in stop_words:
      token = " " + word + " "
      sentence = sentence.replace(token, " ")
    sentences.append(sentence)

# Print to confirm data read correctly
print(len(labels))
print(len(sentences))
print(sentences[0])

# Split data into training and validation sets
train_size = int(len(sentences) * training_portion)

train_sentences = sentences[:train_size]
train_labels = labels[:train_size]

validation_sentences = sentences[train_size:]
validation_labels = labels[train_size:]

# Print lengths to verify splits
print(train_size)
print(len(train_sentences))
print(len(train_labels))
print(len(validation_sentences))
print(len(validation_labels))

# Text Tokenization and Padding
tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_sentences)
# Dictionary mapping words to their index
word_index = tokenizer.word_index

# Convert training and validation sentences to sequences of word indices
train_sequences = tokenizer.texts_to_sequences(train_sentences)
train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)

# Debug print to check a few padded sequences
print(len(train_sequences[0]))
print(len(train_padded[0]))

print(len(train_sequences[1]))
print(len(train_padded[1]))

print(len(train_sequences[10]))
print(len(train_padded[10]))

# Convert validation sentences to padded sequences
validation_sequences = tokenizer.texts_to_sequences(validation_sentences)
validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)

# Debug print
print(len(validation_sequences))
print(validation_padded.shape)

# Encode text labels to integers
label_tokenizer = Tokenizer()
# Fit on full label set to get consistent index mapping
label_tokenizer.fit_on_texts(labels)

# Convert label strings to sequences of integers, then convert to NumPy arrays
training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))
validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))

# Debug print
print(training_label_seq[0])
print(training_label_seq[1])
print(training_label_seq[2])
print(training_label_seq.shape)

print(validation_label_seq[0])
print(validation_label_seq[1])
print(validation_label_seq[2])
print(validation_label_seq.shape)

# Initialize the model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size,
                              embedding_dim,
                              input_shape=(max_length,)),
    tf.keras.layers.GlobalAveragePooling1D(), # Average the embedding vectors across all words
    tf.keras.layers.Dense(24, activation='relu'), # Hidden layer with ReLU
    tf.keras.layers.Dense(6, activation='softmax') # Output layer with 6 categories
])

# Compile the model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# Print model summary
model.summary()

# Train the model
# Initialize no. of epochs
num_epochs = 30

history = model.fit(
    train_padded,
    training_label_seq,
    epochs=num_epochs,
    validation_data=(validation_padded, validation_label_seq),
    verbose=2
)

# Plot accuracy and loss graphs over epochs
def plot_graphs(history, metric_name):
  plt.plot(history.history[metric_name])
  plt.plot(history.history['val_' + metric_name])
  plt.xlabel("Epochs")
  plt.ylabel(metric_name.capitalize())
  plt.title(f"Training vs Validation {metric_name.capitalize()}")
  plt.legend([metric_name, 'val_' + metric_name])
  plt.grid(True)
  plt.show()

plot_graphs(history, "accuracy")
plot_graphs(history, "loss")